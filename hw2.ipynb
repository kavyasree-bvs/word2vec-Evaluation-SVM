{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2019\n",
    "\n",
    "\n",
    "# Homework 2:   word2vec + SVM + Evaluation\n",
    "\n",
    "### 100 points [6% of your final grade]\n",
    "\n",
    "### Due: Tuesday, February 26, 2019 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Understand word2vec-like term embeddings,  explore real-world challenges with SVM-based classifiers, understand and implement several evaluation metrics.\n",
    "\n",
    "*Submission instructions (eCampus):* To submit your homework, rename this notebook as `UIN_hw2.ipynb`. For example, my homework submission would be something like `555001234_hw2.ipynb`. Submit this notebook via eCampus (look for the homework 2 assignment there). Your notebook should be completely self-contained, with the results visible in the notebook. We should not have to run any code from the command line, nor should we have to run your code within the notebook (though we reserve the right to do so). So please run all the cells for us, and then submit.\n",
    "\n",
    "*Late submission policy:* For this homework, you may use as many late days as you like (up to the 5 total allotted to you).\n",
    "\n",
    "*Collaboration policy:* You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by **filling out the Collaboration Declarations at the bottom of this notebook**. \n",
    "\n",
    "*Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2.*\n",
    "\n",
    "The basic rule is that no student should explicitly share a solution with another student (and thereby circumvent the basic learning process), but it is okay to share general approaches, directions, and so on. If you feel like you have an issue that needs clarification, feel free to contact either me or the TA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM (80 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). As in Homework 1, you'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len 27813\n",
      "13938 13875\n",
      "test_len 11920\n",
      "5945 5975\n"
     ]
    }
   ],
   "source": [
    "# Please load the dataset\n",
    "# Your code below\n",
    "\n",
    "\n",
    "train_relevant={}\n",
    "train_irrelevant={}\n",
    "test_relevant={}\n",
    "test_irrelevant={}\n",
    "\n",
    "train_len=0\n",
    "import json\n",
    "with open('review_train.json') as f:\n",
    "    for line in f:\n",
    "        train_len += 1\n",
    "        data = json.loads(line)\n",
    "        review_data = data['review'].split()\n",
    "        id = data['id']\n",
    "        label = data['label']\n",
    "        if(label == 1):\n",
    "            train_relevant[id] = review_data\n",
    "        else:\n",
    "            train_irrelevant[id] = review_data\n",
    "\n",
    "test_len = 0\n",
    "with open('review_test.json') as f:\n",
    "    for line in f:\n",
    "        test_len += 1\n",
    "        data = json.loads(line)\n",
    "        review_data = data['review'].split()\n",
    "        id = data['id']\n",
    "        label = data['label']\n",
    "        if(label == 1):\n",
    "            test_relevant[id] = review_data\n",
    "        else:\n",
    "            test_irrelevant[id] = review_data\n",
    "print('train_len', train_len)\n",
    "print(len(train_relevant), len(train_irrelevant))\n",
    "print('test_len', test_len)\n",
    "print(len(test_relevant), len(test_irrelevant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the pre-trained term embeddings\n",
    "import numpy as np\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    #model = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "    #       for line in lines}\n",
    "    model={}\n",
    "    for line in lines:\n",
    "        split_w = line.split()\n",
    "        key = split_w[0]\n",
    "        val = [float(i) for i in split_w[1:]]\n",
    "        model[key] = np.array(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27813, 50)\n",
      "27813\n"
     ]
    }
   ],
   "source": [
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "# Your code below\n",
    "import numpy as np\n",
    "i = 0\n",
    "\n",
    "def GetMeans(dataset):\n",
    "    h = {}\n",
    "    for key,words in dataset.items():\n",
    "        vec = np.zeros(50,)\n",
    "        for i in range(0,len(words)):\n",
    "            word = bytes(bytearray(words[i], 'utf-8'))\n",
    "            if word not in model:\n",
    "                continue\n",
    "            vec = np.add(vec , model[word])\n",
    "        #h[key] = np.mean(vec)\n",
    "        h[key] = [x/len(words) for x in vec]\n",
    "    return h\n",
    "\n",
    "\n",
    "relevant_means = GetMeans(train_relevant)\n",
    "irrelevant_means = GetMeans(train_irrelevant)\n",
    "\n",
    "def AppendMeans(m, val, mean_vals, Y_vals):  \n",
    "    for k,v in m.items():\n",
    "        mean_vals.append(v)\n",
    "        Y_vals.append(val)\n",
    "    return [mean_vals,Y_vals]\n",
    "\n",
    "[a,b] = AppendMeans(relevant_means, 1, [], [])\n",
    "[train_means,train_Y] = AppendMeans(irrelevant_means, 0, a, b)\n",
    "\n",
    "#print(train_means[:3])\n",
    "train_means=np.array(train_means)\n",
    "print(train_means.shape)\n",
    "print(len(train_Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "With the vector representations you get for each review, please train an SVM model to predict whether a given review is food-relevant or not. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.** You can only train your model with reviews in *review_train.json*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# SVM model training\n",
    "# Your code here\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale', kernel = 'rbf')\n",
    "print(clf.fit(train_means, train_Y))\n",
    "#clf.score(np.array(train_means), train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict whether a given review is food-relevant or not. Please report the overall accuracy, precision and recall of your model on the **testing data**. You should **implement the functions for accuracy, precision, and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained term embeddings:\n",
      "Accuracy 0.9034395973154362\n",
      "Precision 0.87999365884591\n",
      "Recall 0.9337258200168208\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "[c,d] = AppendMeans(GetMeans(test_irrelevant), 0, [], [])\n",
    "[test_means,test_Y] = AppendMeans(GetMeans(test_relevant), 1, c, d)\n",
    "\n",
    "predicted = np.array(clf.predict(test_means))\n",
    "\n",
    "diff = [np.abs(a-b) for a,b in zip(test_Y, predicted)]\n",
    "\n",
    "accuracy = 1-(diff.count(1)/len(predicted))\n",
    "\n",
    "actual = test_Y\n",
    "\n",
    "fneg = 0\n",
    "fpos = 0\n",
    "tpos =0\n",
    "for i in range(len(actual)):\n",
    "    if(actual[i]-predicted[i] == 1):\n",
    "        fneg+=1\n",
    "    if(predicted[i]-actual[i] == 1):\n",
    "        fpos+=1\n",
    "    if(predicted[i]+actual[i] == 2):\n",
    "        tpos+=1\n",
    "print('Pre-trained term embeddings:')\n",
    "print('Accuracy', accuracy)\n",
    "P= tpos/(tpos+fpos)\n",
    "R= tpos/(tpos+fneg)\n",
    "print('Precision', P)\n",
    "print('Recall', R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-based embeddings\n",
    "\n",
    "Instead of taking the mean of term embeddings, you can directly train a **doc2vec** model for paragraph or document embeddings. You can refer to the paper [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf) for more details. And in this homework, you can make use of the implementation in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "Now, you need to:\n",
    "* Train a doc2vec model based on all reviews you have (training + testing sets).\n",
    "* Use the embeddings from your doc2vec model to represent each review and train a new SVM model.\n",
    "* Report the overall accuracy, precision and recall of your model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kavya\\Anaconda2\\envs\\cs670\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "# Train a doc2vec\n",
    "# Your code here\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #print(line)\n",
    "            data = json.loads(line)\n",
    "            review_data = data['review']\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(review_data), [i])\n",
    "            '''    \n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(review_data)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(review_data), [i])\n",
    "            '''\n",
    "train_corpus = list(read_corpus('review_train.json'))\n",
    "test_corpus = list(read_corpus('review_test.json', tokens_only=True))\n",
    "epochs_count = 100\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, epochs=100)\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training SVM\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Train a SVM\n",
    "# Your code here\n",
    "def GetWords(dataset):\n",
    "    h = {}\n",
    "    for key,words in dataset.items():\n",
    "        vec = np.zeros(50,)\n",
    "        vec = np.add(vec, model.infer_vector(words))\n",
    "        h[key] = vec\n",
    "    return h\n",
    "\n",
    "def Append(m, val, mean_vals, Y_vals):  \n",
    "    for k,v in m.items():\n",
    "        mean_vals.append(v)\n",
    "        Y_vals.append(val)\n",
    "    return [mean_vals,Y_vals]\n",
    "\n",
    "[a,b] = Append(GetWords(train_relevant), 1, [], [])\n",
    "[train_vecs,train_vec_Y] = Append(GetWords(train_irrelevant), 0, a, b)\n",
    "\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='scale', kernel='rbf')\n",
    "print('training SVM')\n",
    "print(clf.fit(train_vecs, train_vec_Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim Doc2Vec: epochs = 100\n",
      "Accuracy 0.9046979865771811\n",
      "Precision 0.8975037196230782\n",
      "Recall 0.9132043734230446\n"
     ]
    }
   ],
   "source": [
    "# Report the performance\n",
    "# Your code here\n",
    "\n",
    "[c1,d1] = Append(GetWords(test_irrelevant), 0, [], [])\n",
    "[test_vecs,test_vec_Y] = Append(GetWords(test_relevant), 1, c1, d1)\n",
    "\n",
    "predicted = np.array(clf.predict(test_vecs))\n",
    "\n",
    "diff = [np.abs(a-b) for a,b in zip(test_vec_Y, predicted)]\n",
    "\n",
    "accuracy = 1-(diff.count(1)/len(predicted))\n",
    "\n",
    "actual = test_vec_Y\n",
    "\n",
    "fneg = 0\n",
    "fpos = 0\n",
    "tpos =0\n",
    "for i in range(len(actual)):\n",
    "    if(actual[i]-predicted[i] == 1):\n",
    "        fneg+=1\n",
    "    if(predicted[i]-actual[i] == 1):\n",
    "        fpos+=1\n",
    "    if(predicted[i]+actual[i] == 2):\n",
    "        tpos+=1\n",
    "print('Gensim Doc2Vec: epochs =', epochs_count)\n",
    "print('Accuracy', accuracy)\n",
    "P= tpos/(tpos+fpos)\n",
    "R= tpos/(tpos+fneg)\n",
    "print('Precision', P)\n",
    "print('Recall', R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the term-based average approach vs. the doc2vec approach? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained term embeddings:\n",
    "Accuracy 0.9034395973154362\n",
    "Precision 0.87999365884591\n",
    "Recall 0.9337258200168208\n",
    "\n",
    "Gensim Doc2Vec: epochs = 100\n",
    "Accuracy 0.9046979865771811\n",
    "Precision 0.8975037196230782\n",
    "Recall 0.9132043734230446\n",
    "\n",
    "From the above results, I observed that Doc2Vec is more accurate and precise than pre-trained term embeddings. I believe this is because of the 50 dimensional weights being decided by using the train and test data itself in Doc2Vec. \n",
    "\n",
    "On the other hand, recall value is higher for pre-trained term embeddings. Recall value tells how accurately we can predict the positives. I believe since the pre-term embeddings give an original distribution without getting a glimpse of the train or the test data, it would predict the positives better than word2vec where the weights are biased based on the train&test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you do better?\n",
    "\n",
    "Finally, see if you can do better than either the word- or doc- based embeddings approach for classification. You may explore new features, new classifiers, etc. Whatever you like. Just provide your code and a justification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training the model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_vecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3acdf7dc9247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'training the model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_vecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_vec_Y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpredict_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_vecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_vecs' is not defined"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "print('training the model')\n",
    "'''\n",
    "X = train_vecs\n",
    "y = train_vec_Y\n",
    "predict_X = test_vecs\n",
    "actual = test_vec_Y\n",
    "'''\n",
    "X = train_means\n",
    "y = train_Y\n",
    "predict_X = test_means\n",
    "actual = test_Y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def GetAccuracy(classifier):\n",
    "    print('---------')\n",
    "    #print(\n",
    "    print(classifier.fit(X,y))\n",
    "    predicted = np.array(classifier.predict(predict_X))\n",
    "\n",
    "    diff = [np.abs(a-b) for a,b in zip(actual, predicted)]\n",
    "    accuracy = 1-(diff.count(1)/len(predicted))\n",
    "\n",
    "    fneg = 0\n",
    "    fpos = 0\n",
    "    tpos =0\n",
    "    for i in range(len(actual)):\n",
    "        if(actual[i]-predicted[i] == 1):\n",
    "            fneg+=1\n",
    "        if(predicted[i]-actual[i] == 1):\n",
    "            fpos+=1\n",
    "        if(predicted[i]+actual[i] == 2):\n",
    "            tpos+=1\n",
    "    print('Accuracy', accuracy)\n",
    "    \n",
    "    P= tpos/(tpos+fpos)\n",
    "    R= tpos/(tpos+fneg)\n",
    "    print('Precision', P)\n",
    "    print('Recall', R)\n",
    "    print('---------')\n",
    "\n",
    "'''\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "GetAccuracy(clf)\n",
    "clf = QuadraticDiscriminantAnalysis()\n",
    "GetAccuracy(clf)\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "GetAccuracy(clf)\n",
    "clf = RandomForestClassifier()\n",
    "GetAccuracy(clf)\n",
    "clf = AdaBoostClassifier()\n",
    "GetAccuracy(clf)\n",
    "clf = GradientBoostingClassifier()\n",
    "GetAccuracy(clf)\n",
    "clf = GaussianNB()\n",
    "GetAccuracy(clf)\n",
    "clf = SVC(kernel=\"rbf\")\n",
    "GetAccuracy(clf)\n",
    "clf = NuSVC(probability=True)\n",
    "GetAccuracy(clf)\n",
    "'''\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "GetAccuracy(clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After trying out with various available classifiers and measuring their accuracy. I realized that the closest accuracy I could get to the previous results is of LinearDiscriminantAnalysis with accuracy = 0.8926174496644296\n",
      "\n",
      "I believe tweaking the parameters of LDA would help get better accuracy.\n"
     ]
    }
   ],
   "source": [
    "print('After trying out with various available classifiers and measuring their accuracy. I realized that the closest accuracy I could get to the previous results is of LinearDiscriminantAnalysis with accuracy = 0.8926174496644296')\n",
    "print()\n",
    "print('I believe tweaking the parameters of LDA would help get better accuracy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: NDCG (20 points)\n",
    "\n",
    "You calculated the recall and precision in Part 1 and now you get a chance to implement NDCG. \n",
    "\n",
    "Assume that Amy searches for \"food-relevant\" reviews in the **testing set** on two search engines `A` and `B`. Since the ground-truth labels for the reviews are unknown to A and B, they need to make a prediction for each review and then return a ranked list of results based on their probabilities. The results from A are in *search_result_A.json*, and the results from B are in *search_result_B.json*. Each line contains the id of a review and its corresponding ranking.\n",
    "\n",
    "You can check their labels in *review_test.json* while calculating the NDCG scores. If a review is \"food-relevant\", the relevance score is 1. Otherwise, the relevance score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg for search result A: 0.96916958684041\n"
     ]
    }
   ],
   "source": [
    "# NDCG for search_result_A.json\n",
    "# Your code here\n",
    "from operator import itemgetter\n",
    "\n",
    "def GetNdcg(filename):\n",
    "    my_list = []\n",
    "    with open(filename) as f:\n",
    "        dcg=0\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            review_id = data['id']\n",
    "            i = data['rank']\n",
    "            label=0\n",
    "            if review_id in test_relevant:\n",
    "                label=1\n",
    "            my_list.append([review_id, label])\n",
    "            dcg += label/np.log2(1+i)\n",
    "\n",
    "    ideal_list = sorted(my_list, key=itemgetter(1), reverse=True)\n",
    "    idcg=0\n",
    "    for j in range(len(ideal_list)):\n",
    "        [review_id, label] = ideal_list[j]\n",
    "        i = j+1\n",
    "        idcg += label/np.log2(1+i)\n",
    "    ndcg = dcg/idcg\n",
    "    \n",
    "    return ndcg\n",
    "\n",
    "print('ndcg for search result A:', GetNdcg('search_result_A.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg for search result B: 0.9972073294831962\n"
     ]
    }
   ],
   "source": [
    "# NDCG for search_result_B.json\n",
    "# Your code here\n",
    "print('ndcg for search result B:', GetNdcg('search_result_B.json'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you collaborated with anyone (see Collaboration policy at the top of this homework), you can put your collaboration declarations here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I followed doc2vec tutorial from here:\n",
    "    https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
